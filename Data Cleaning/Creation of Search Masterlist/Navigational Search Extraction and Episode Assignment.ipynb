{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b6b31d",
   "metadata": {},
   "source": [
    "# Navigational Search Pipeline: Masterlist Construction\n",
    "\n",
    "This notebook processes school-level search logs, identifies navigational queries, and constructs a masterlist of searches for all schools. Each (`device_name_actual`, `school_name`) pair is treated as a distinct user to preserve school-specific analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b8dec",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Settings and Imports\n",
    "\n",
    "We define file paths, keywords, and suffixes for the school search CSVs. Necessary libraries are imported.\n",
    "- BASE_PATH: Folder containing cleaned CSVs for each school.\n",
    "\n",
    "- NAV_DICT_FILE: List of known navigational sites.\n",
    "\n",
    "- KEYWORDS: Words added to site names to catch variations (online, web, login, channel).\n",
    "\n",
    "- FILE_SUFFIX: The suffix of cleaned school CSVs (_all_searches_tagged.csv).\n",
    "\n",
    "- OUTPUT_MASTER: Path to save the final master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# =====================================================\n",
    "# SETTINGS\n",
    "# =====================================================\n",
    "BASE_PATH = \"/Users/tdf/Downloads/q_episode_processing/cleaned\"  # Folder with school subfolders\n",
    "NAV_DICT_FILE = \"/Users/tdf/Downloads/navigational_dictionary.csv\"\n",
    "OUTPUT_MASTER = \"/Users/tdf/Downloads/q_episode_processing/master_all_schools.csv\"\n",
    "KEYWORDS = [\"online\", \"web\", \"login\", \"channel\"]\n",
    "FILE_SUFFIX = \"_all_searches_tagged.csv\"  # Each school's cleaned CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8739ce4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Navigational Terms\n",
    "\n",
    "We build a comprehensive list of navigational search terms using the provided dictionary, padding each site name with keywords such as `\"online\"`, `\"web\"`, `\"login\"`, and `\"channel\"`. Terms are normalized to lowercase for case-insensitive matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_dict = pd.read_csv(NAV_DICT_FILE)\n",
    "nav_terms = []\n",
    "\n",
    "for site in nav_dict['site_name'].dropna().unique():\n",
    "    nav_terms.append(site.lower())\n",
    "    for kw in KEYWORDS:\n",
    "        nav_terms.append(f\"{site.lower()} {kw}\")\n",
    "\n",
    "nav_terms = list(set(nav_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617383e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Helper Functions\n",
    "\n",
    "Two key functions are defined:\n",
    "\n",
    "- **`extract_query`(uri)**: Extracts the search query from a URI and normalizes it to lowercase.\n",
    "\n",
    "- **`is_navigational`(query)**: Flags a query as navigational if it matches any term in the expanded dictionary using fuzzy or substring matching.\n",
    "\n",
    "- **`is_utility_query`(query)**: Flags utility URLs (/url, uviewer, ogs.google) which are excluded from navigational classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query(uri):\n",
    "    \"\"\"Extract the search query from a URI, normalize to lowercase.\"\"\"\n",
    "    if pd.isna(uri):\n",
    "        return \"\"\n",
    "    match = re.search(r\"[?&]q=([^&]+)\", uri)\n",
    "    if match:\n",
    "        return match.group(1).replace('+', ' ').lower()\n",
    "    return str(uri).lower()\n",
    "\n",
    "def is_navigational(query):\n",
    "    \"\"\"Return True if query matches any navigational term.\"\"\"\n",
    "    for term in nav_terms:\n",
    "        if fuzz.partial_ratio(query, term) >= 65:\n",
    "            return True\n",
    "        if term in query:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "utility_patterns = [r'/url', r'uviewer', r'ogs\\.google']\n",
    "\n",
    "def is_utility_query(query):\n",
    "    \"\"\"Return True if query is a utility URL that should be ignored.\"\"\"\n",
    "    return any(re.search(pattern, query) for pattern in utility_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7b4fa",
   "metadata": {},
   "source": [
    "## 4. Process School Files\n",
    "\n",
    "For each school folder:\n",
    "\n",
    "- Load the cleaned CSV (`*_all_searches_tagged.csv`).\n",
    "\n",
    "- Add the school_name column to ensure that the same device_name_actual across schools is treated as distinct users.\n",
    "\n",
    "- Extract the query, remove duplicates, classify navigational searches, and assign q_episode.\n",
    "\n",
    "- Classify navigational queries (`is_navigational`).\n",
    "\n",
    "- Sort by device and timestamp for episode assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2437bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for school_folder in os.listdir(BASE_PATH):\n",
    "    folder_path = os.path.join(BASE_PATH, school_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.endswith(FILE_SUFFIX):\n",
    "            continue\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        print(\"Loading:\", full_path)\n",
    "\n",
    "        df = pd.read_csv(full_path)\n",
    "\n",
    "        # Add school_name column (important for identity)\n",
    "        df['school_name'] = school_folder\n",
    "\n",
    "        # Extract query and remove duplicates\n",
    "        df['search_q'] = df['uri'].apply(extract_query)\n",
    "        df = df.drop_duplicates(subset=['device_name_actual', 'search_q', 'created_at'])\n",
    "\n",
    "        # Classify navigational searches, excluding utility URLs\n",
    "        df['is_navigational'] = df['search_q'].apply(\n",
    "            lambda q: is_navigational(q) and not is_utility_query(q)\n",
    "        )\n",
    "\n",
    "        # Sort for q_episode calculation\n",
    "        df = df.sort_values(['device_name_actual', 'created_at'])\n",
    "\n",
    "        # Initialize q_episode\n",
    "        df['q_episode'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55413411",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Episode Assignment\n",
    "\n",
    "- `q_episode` = 0 for navigational and utility queries.\n",
    "\n",
    "- **Non-navigational queries**:\n",
    "    - Queries **within 5 minutes** of the last meaningful query remain in the **same episode**.\n",
    "    - Queries **more than 5 minutes** apart start a **new episode**.\n",
    "- Episode counter resets per (`device_name_actual`, `school_name`), so the same device in different schools is treated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Assign q_episode per user per school\n",
    "        for (device_id, school), user_data in df.groupby(['device_name_actual', 'school_name']):\n",
    "            last_time = None\n",
    "            episode = 1\n",
    "            for idx, row in user_data.iterrows():\n",
    "                if row['is_navigational']:\n",
    "                    df.at[idx, 'q_episode'] = 0\n",
    "                else:\n",
    "                    if last_time is None:\n",
    "                        df.at[idx, 'q_episode'] = episode\n",
    "                    else:\n",
    "                        delta = (pd.to_datetime(row['created_at']) - pd.to_datetime(last_time)).total_seconds()\n",
    "                        if delta > 5*60:  # New episode if gap > 5 min\n",
    "                            episode += 1\n",
    "                        df.at[idx, 'q_episode'] = episode\n",
    "                    last_time = row['created_at']\n",
    "\n",
    "        all_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d3e9d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Concatenate All Schools\n",
    "\n",
    "Combine all individual school dataframes into a master dataframe for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19095f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"Master dataframe shape:\", master_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d427e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export Master File\n",
    "\n",
    "Save the master dataframe to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ab0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv(OUTPUT_MASTER, index=False)\n",
    "print(\"Saved master file to:\", OUTPUT_MASTER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
